{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import statistics\n",
    "import string\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "DATA_DIR = r\"Gutenberg/actual/\"\n",
    "DATABASE_USER = 'root'\n",
    "DATABASE_USER_PASSWORD = 'password'\n",
    "DATABASE_HOST = 'localhost'\n",
    "DATABASE_NAME = 'Authorship_Attribution'\n",
    "DATABASE_TABLE = 'author1'\n",
    "DATABASE_CONNECTION = \"mysql+pymysql://\" + DATABASE_USER + \":\" + DATABASE_USER_PASSWORD + \"@\" + DATABASE_HOST + \"/\" + DATABASE_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing():\n",
    "    author, file_path = get_author_and_file_path()\n",
    "    authors, text = get_authors_and_text_list(author, file_path)\n",
    "    return authors, text\n",
    "\n",
    "\n",
    "def get_author_and_file_path():\n",
    "    author, text = [], []\n",
    "    for file in glob.glob(f\"{DATA_DIR}*.txt\"):\n",
    "        author.append(file.split(\"/\")[-1].split(\"__\")[0])\n",
    "        text.append(file)\n",
    "    return author, text\n",
    "\n",
    "\n",
    "def get_authors_and_text_list(author, file_path):\n",
    "    authors, text = [], []\n",
    "    for auth, file in zip(author, file_path):\n",
    "        name, sentence = get_author_and_text(auth, file)\n",
    "        authors.extend(name)\n",
    "        text.extend(sentence)\n",
    "    return authors, text\n",
    "\n",
    "\n",
    "def get_author_and_text(author, file):\n",
    "    authors, text = [], []\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = data.strip()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    for para in data:\n",
    "        authors.append(author)\n",
    "        para = para.replace(\"\\n\", \"\")\n",
    "        text.append(para)\n",
    "    return authors, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(paragraph):\n",
    "    feature_set = []\n",
    "    unique_word = set()\n",
    "    stop_words = 0\n",
    "    commas = 0\n",
    "    special_char = 0\n",
    "    uppercase = 0\n",
    "    articles = 0\n",
    "    nouns = 0\n",
    "    verbs = 0\n",
    "    pronouns = 0\n",
    "\n",
    "    token_word = nltk.word_tokenize(paragraph)\n",
    "    tagged_word = nltk.pos_tag(token_word)\n",
    "\n",
    "    para = (len(paragraph))\n",
    "    sent = paragraph.count('.')\n",
    "    sent_max_len = len(max(paragraph.split('.')))\n",
    "    words = (len(token_word))\n",
    "\n",
    "    for word, tag in tagged_word:\n",
    "        if word not in unique_word and tag not in ('AT', 'DT') and \\\n",
    "                word not in stopwords.words('english') and word not in string.punctuation:\n",
    "            unique_word.add(word)\n",
    "        if word in stopwords.words('english'):\n",
    "            stop_words += 1\n",
    "        if word in string.punctuation:\n",
    "            if word == ',':\n",
    "                commas += 1\n",
    "            else:\n",
    "                special_char += 1\n",
    "        if word.isupper():\n",
    "            uppercase += 1\n",
    "        # if tag in ('AT', 'DT'):\n",
    "        #     articles += 1\n",
    "        # if tag in ('NNP', 'NOUN'):\n",
    "        #     nouns += 1\n",
    "        # if tag in ('VBD', 'VERB'):\n",
    "        #     verbs += 1\n",
    "        # if tag in ('PRP', 'PRON'):\n",
    "        #     pronouns += 1\n",
    "\n",
    "    feature_set.append(para)\n",
    "    feature_set.append(sent)\n",
    "    feature_set.append(sent_max_len)\n",
    "    feature_set.append(words)\n",
    "    feature_set.append(len(unique_word))\n",
    "    feature_set.append(stop_words)\n",
    "    feature_set.append(commas)\n",
    "    feature_set.append(special_char)\n",
    "    feature_set.append(uppercase)\n",
    "    # feature_set.append(articles)\n",
    "    # feature_set.append(nouns)\n",
    "    # feature_set.append(verbs)\n",
    "    # feature_set.append(pronouns)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_database():\n",
    "    engine = create_engine(DATABASE_CONNECTION)\n",
    "    return engine\n",
    "\n",
    "\n",
    "def disconnect_to_database(engine):\n",
    "    engine.dispose()\n",
    "\n",
    "\n",
    "def write_to_database(engine, df):\n",
    "    df.to_sql(con=engine, name=DATABASE_TABLE, if_exists='replace', chunksize=5000)\n",
    "\n",
    "\n",
    "def read_from_database(engine):\n",
    "    df1 = pd.DataFrame()\n",
    "    df1 = pd.read_sql('SELECT * FROM author1', con=engine)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_cloud(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(set(STOPWORDS))\n",
    "\n",
    "    for name, group in df:\n",
    "        temp_sent = ' '.join(group['sentences'])\n",
    "        word_cloud = WordCloud(max_font_size=50, stopwords=stop_words,\n",
    "                               relative_scaling=0.5, normalize_plurals=False).generate(temp_sent)\n",
    "        plt.imshow(word_cloud, interpolation='bilinear')\n",
    "        plt.title(name)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(f\"{name}.png\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, y_train, X_test, y_test, classes):\n",
    "    gnb = MultinomialNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    print(f\"Naive Bayes - Accuracy: {100 * accuracy_score(y_test, y_pred):2.4f}%\")\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    display = plot_confusion_matrix(gnb, X_test, y_test, display_labels=classes, cmap=plt.cm.Blues, normalize='true')\n",
    "    display.ax_.set_title('Confusion matrix')\n",
    "    plt.savefig(\"Naive_Bayes.png\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train, X_test, y_test, classes):\n",
    "    svm_clf = svm.SVC(kernel='linear')\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "    print(f\"SVM - Accuracy: {100 * accuracy_score(y_test, y_pred):2.4f}%\")\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    display = plot_confusion_matrix(svm_clf, X_test, y_test, display_labels=classes, cmap=plt.cm.Blues, normalize='true')\n",
    "    display.ax_.set_title('Confusion matrix')\n",
    "    plt.savefig(\"SVM.png\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pairwise_comparison(df):\n",
    "    sns_plot = sns.pairplot(data=df, hue=\"authors\")\n",
    "    sns_plot.savefig(\"Pairwise_Comparison_of_Features.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_print_optimal_k(X):\n",
    "    wass = []\n",
    "    for i in range(1, 21):\n",
    "        KM = KMeans(init='k-means++', n_clusters=i, max_iter=500)\n",
    "        KM.fit(X)\n",
    "        wass.append(KM.inertia_)\n",
    "\n",
    "    plt.plot(range(1, 21), wass, color='green', linewidth='3')\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sqaured Error (wass)\")\n",
    "    plt.savefig(\"Optimal-k.png\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def kmeans(X, y, cluster):\n",
    "    # print no of optimal K\n",
    "    kmeans_print_optimal_k(X)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=cluster)\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=100, cmap='viridis')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=100, alpha=0.5)\n",
    "    plt.savefig(\"k-Means.png\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def kmeans_print_accuracy(y_actual, y_predict, kmeans_type):\n",
    "    accuracy_count = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] == y_predict[i]:\n",
    "            accuracy_count += 1\n",
    "    print(f\"K-Means - {kmeans_type} Accuracy: {100 * accuracy_count / len(y_actual):2.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_classification(X, y, cluster):\n",
    "    X = X.to_numpy()\n",
    "    labelEncoder = LabelEncoder()\n",
    "    y_encoded = labelEncoder.fit_transform(y)\n",
    "    #     X_actual = X\n",
    "    X_actual, y_actual = shuffle(X, y_encoded, random_state=0)\n",
    "\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=cluster)\n",
    "    kmeans.fit(X_actual)\n",
    "    y_kmeans = kmeans.predict(X_actual)\n",
    "    kmeans_print_accuracy(y_actual, y_kmeans, 'Standard')\n",
    "\n",
    "    # max scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    kmeans.fit(X_scaled)\n",
    "    y_scaled = kmeans.predict(X_scaled)\n",
    "    kmeans_print_accuracy(y_actual, y_scaled, 'With Scaling')\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=cluster)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    kmeans.fit(X_pca)\n",
    "    y_pca = kmeans.predict(X_pca)\n",
    "    kmeans_print_accuracy(y_actual, y_pca, 'With PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "authors, sentences = pre_processing()\n",
    "df = pd.DataFrame()\n",
    "df['authors'], df['sentences'] = authors, sentences\n",
    "classes = list(set(authors))\n",
    "del authors\n",
    "del sentences\n",
    "\n",
    "# Get all features\n",
    "df['features'] = df['sentences'].apply(lambda sentence: get_features(sentence))\n",
    "df[['para_len', 'sent', 'sent_max_len', 'word', 'unique_word', 'stop_words', 'comma', 'special', 'uppercase']] = \\\n",
    "    pd.DataFrame(df.features.values.tolist(), index=df.index)\n",
    "del df['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word cloud\n",
    "print_word_cloud(df.groupby('authors'))\n",
    "\n",
    "# Write to DB\n",
    "connection = connect_to_database()\n",
    "write_to_database(connection, df)\n",
    "\n",
    "# Read from DB\n",
    "# df1 = pd.DataFrame()\n",
    "# df1 = read_from_database(connection)\n",
    "\n",
    "# X, y and split\n",
    "X, y = df.iloc[:, 3:], df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "# Naive Bayes\n",
    "naive_bayes(X_train, y_train, X_test, y_test, classes)\n",
    "# SVM\n",
    "support_vector_machine(X_train, y_train, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "# K-Means pairwise\n",
    "kmeans_pairwise_comparison(df)\n",
    "# K-Means clustering\n",
    "kmeans(X, y, len(classes))\n",
    "# K-Means classification\n",
    "kmeans_classification(X, y, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
